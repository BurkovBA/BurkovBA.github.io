---
title: Variational Autoencoder (VAE)
date: "2022-12-31T00:00:00.284Z"
tags: ["math", "programming"]
cover: "./vae.png"
description: Here I discuss one of the two most popular classes of generative models for creating images. 
---

## Autoencoders

The concept of Autoencoders stems from the information theory. 

TODO: Y. Bengio, denoising autoencoders, stacked denoising autoencoders.

##  Variational autoencoder (VAE)

VAE, devised by Max Welling group, is formulated in Bayesian terms. The idea of this approach starts from the information
theory perspective: we want to train such an encoder neural network $\mathcal{E}_{\phi}$ with parameters $\phi$ that the 
Kullback-Liebler divergence between the input image $\bf x$ and its latent representation $\bf z$ is minimized:
$KL({\bf x}, {\bf z}) \to min$.

Computationally VAE minimizes the divergence using stochastic gradient descent. However, as we'll see later using
a regular neural network training approach does not get the job done, as normal gradient estimator has a very high 
variance and does not converge computationally (we'll see this in a moment).

Hence, training VAE employs a specific computational technique, called doubly-stochastic gradient descent and a special
trick, called re-parametrization trick, which we will explore later.

But first we need to understand the language, in which VAE is described, as it is a Bayesian model, and we will have to 
cover a lot of background in Bayesian ML.

![Bayes formula](bayes.png)<center>**All hail our lord and saviour Bayes...** (meh, just kidding)</center>

### Bayes formula

To explain the Bayesian framework, employed by VAE, we have to start with Bayes formula:

$\underbrace{ p(z | x) }_\text{posterior} = \frac{ \overbrace{ p(x | z)}^\text{evidence} \cdot \overbrace{p(z)}^{prior} }{ \underbrace{ p(x) }_\text{evidence} }$

In case of VAE the notation is as follows:

* we have a dataset of images $X = \{ {\bf x^{(i)}} \}$ 
* each input image is denoted $\bf x^{(i)}$
* its latent representation, which is generated by VAE's encoder half, is denoted $\bf z^{(i)}$
* the weights of encoder network $\mathcal{E}$ are denoted $\phi$; Kingma and Welling call them **variational parameters**
* the weights of decoder network $\mathcal{D}$ are denoted $\theta$; Kingma and Welling call them **generative parameters**

The authors of VAE, D. Kingma and M. Welling, assume that there exists some prior distribution of latent parameters $p({\bf z})$, from which latent 
representation of each data point is sampled. For each image ${\bf x}$ we maximize the posterior $p_{\phi}(z|x)$.

### Variational inference

Direct calculation of posterior $p({\bf z} | {\bf x})$ using Bayes formula is impossible, as we need to calculate the probability of evidence $p(x)$, 
for which the integral $p({\bf x}) = \int p({\bf x} | {\bf z}) p({\bf z}) d {\bf z}$ is intractable (i.e. it is not 
possible to calculate it analytically or computationally in practice).

So we need to come up with a practical way of overcoming this obstacle. 

Typically Bayesians have two solutions for problems like this: one solution is Markov Chain Monte Carlo methods. In this
particular case MCMC estimator is time-consuming and gradient, calculated with it, has a high variance, so the model 
fails to converge.

An alternative approach is Variational Inference approach, which we explain here.

![variational inference](variational_inference.png)<center>**Variational inference.** Variational inference aims to approximate the true variational posterior $p({\bf z}|{\bf x})$ with the best approximation $q^*({\bf z})$ from a certain class of functions $Q$. This optimization process minimizes the Kullback-Liebler divergence between the approximation $q({\bf z})$ and true posterior $p({\bf z}|{\bf x})$. Image taken from [Gregory Gundersen blog post](https://gregorygundersen.com/blog/2019/11/10/em/) on Variational Inference.</center>

In variational inference we choose a class of functions $Q$, from which we will try to pick an approximation $q({\bf z})$ 
(called **guide**) of the posterior $p({\bf z} | {\bf x})$, such that Kullback-Liebler divergence between this approximation and true
posterior is minimal.

### ELBO maximization

Now, we need to come up with a technical way to find this optimal guide $q({\bf z})$ numerically.

Out of blue sky we consider $\log p(x)$. Let us do 2 tricks with it, first represent it as an integral, and then split it into 2 terms:

$\log p({\bf x}) = \int q({\bf z}) \log{p(x)} d{\bf z} = \int q({\bf z}) \log \frac{p({\bf x}, {\bf z})}{p({\bf z} | {\bf x})} d {\bf z} = \int q({\bf z}) \log \frac{p({\bf x}, {\bf z}) q({\bf z})}{p( {\bf z} | {\bf x} ) q({\bf z})} d{\bf z} = $

$ = \int q({\bf z}) \log \frac{p({\bf x}, {\bf z})}{q({\bf z})} d{\bf z} + \int q({\bf z}) \log \frac{q({\bf z})}{p({\bf z} | {\bf x})}  d {\bf z} = \mathcal{L}(q({\bf z})) + KL(q({\bf z}) \Vert p({\bf z} | {\bf x}))$.

Now we see that log-evidence $\log p({\bf x})$ consists of two non-negative terms. Let us interpret them: 

$\log p({\bf x}) = \underbrace{ \mathcal{L}(q({\bf z})) }_\text{ELBO - Evidence lower bound} + KL(q({\bf z}) \Vert p({\bf z} | {\bf x}))$

The first term is called **Evidence Lower BOund (ELBO)**. The second term is our cost function $KL(q({\bf z}) \Vert p({\bf z} | {\bf x}))$, 
which Variational Inference aims to minimize. As log-evidence $\log p({\bf x})$ is fixed, the greater ELBO gets, the closer in terms of 
KL divergence guide $q({\bf z})$ approximates the posterior $p({\bf z} | {\bf x})$:

$KL \ge 0 \Rightarrow KL(q({\bf z}) \Vert p({\bf z} | {\bf x})) \to \min \Leftrightarrow \mathcal{L}(q({\bf z})) \to \max$

Hence, to find the optimal guide $q({\bf z})$, in practice we have to maximize ELBO. Let us break it down further:

$\mathcal{L}(q({\bf z})) = \int q({\bf z}) \log \frac{p({\bf x}, {\bf z})}{q({\bf z})} d{\bf z} = \int q({\bf z}) \log \frac{ p({\bf x}|{\bf z}) p({\bf z}) }{q({\bf z})} d{\bf z} = $

$= \int q({\bf z}) \log p({\bf x}, {\bf z}) d{\bf z} + \int q({\bf z}) \log \frac{ p({\bf z}) }{q({\bf z})} d{\bf z} = \underbrace{ \mathbb{E}_{ q({\bf z}) } \log p({\bf x}|{\bf z}) }_\text{Expected log-likelihood} - \underbrace{ KL(q({\bf z}) \Vert p({\bf z}))}_\text{Regulariser term KL-divergence}$

We see that our loss function consists of 2 terms. The first term characterizes the quality of reconstruction. The second
term is a regularizer term that requires that our guide stays relatively close to the prior $p({\bf z})$, which is 
usually chosen to be Gaussian.

### Re-parametrization trick

TODO: explain, how we make parameters of distribution static variables and inject randomness as a fixed distribution.

## Transformers and attention mechanism

For explanation of attention mechansim and transformers, see my [older post on AlphaFold2](/2021-12-25-1).

## Denoising autoencoders

TODO

## Denoising VAE

TODO

## References:
* https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf - Bayesian Learning in Stochastic Gradient Langevin Dynamics by M.Welling, Y.W.Teh
* https://www.researchgate.net/publication/272086159_Static_hand_gesture_recognition_using_stacked_Denoising_Sparse_Autoencoders
* https://gregorygundersen.com/blog/2021/04/16/variational-inference/ - Gregory Gundersen on Variational Inference (VI)
* https://gregorygundersen.com/blog/2019/11/10/em/ - Gregory Gunderson on Variational EM
* https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2 - blog post by Aqeel Anwar on VAE with good images 
* https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/ - blog post on VI and Variational EM
* https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf - stacked denoising autoencoders by Y.Bengio group
* https://www.youtube.com/watch?v=xH1mBw3tb_c - 2019 lecture by D. Vetrov on variational inference
* https://arxiv.org/pdf/1312.6114.pdf - VAE paper by D. Kingma and M. Welling
* https://www.youtube.com/watch?v=9zKuYvjFFS8 - video on AE and VAEs
* https://arxiv.org/pdf/1511.06406.pdf - denoising VAE by Y. Bengio et al.
* https://arxiv.org/pdf/2105.05233.pdf - diffusion models beat GANs by Alex Nichol, Prafulla Dhariwal
* https://arxiv.org/pdf/1606.05328.pdf - pixelCNN encoder paper
* https://arxiv.org/pdf/1711.00937.pdf - VQ-VAE paper
* https://arxiv.org/pdf/2110.03318.pdf - on detecting holes in VAE latent space
